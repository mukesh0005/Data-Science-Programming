{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTree Classifer Demonstration\n",
    "\n",
    "In this tutorial we will demonstrate how to use the `DecisionTreeClassifer` class in `scikit-learn` to perform classifications predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Setup\n",
    "Import modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Load data\n",
    "Load data (it's already cleaned and preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./data/airbnb_train_X_price_gte_150.csv') \n",
    "y_train = pd.read_csv('./data/airbnb_train_y_price_gte_150.csv') \n",
    "X_test = pd.read_csv('./data/airbnb_test_X_price_gte_150.csv') \n",
    "y_test = pd.read_csv('./data/airbnb_test_y_price_gte_150.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=500. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best precision score is 0.9370404920282969\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 0.01, 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'C':[0.1,1,10,100],\n",
    "    'gamma':[1,0.1,0.01,0.001],\n",
    "    'kernel':['poly']\n",
    "    \n",
    "}\n",
    "\n",
    "SVM_R_out = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = SVM_R_out, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best precision score is 0.9370404920282969\n",
      "... with parameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "#grid search in SVM\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'C':[0.1,1,10,100],\n",
    "    'gamma':[1,0.1,0.01,0.001],\n",
    "    'kernel':['poly']\n",
    "    \n",
    "}\n",
    "\n",
    "SVM_G_out = SVC()\n",
    "grid_search = GridSearchCV(estimator = SVM_G_out, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Model the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct an initial random search across a wide range of possible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.8594100052265512\n",
      "... with parameters: {'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0011, 'max_leaf_nodes': 30, 'max_depth': 29, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "60 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.83927765 0.77844887 0.82634772 0.84491204 0.84143592 0.82956749\n",
      " 0.82488631 0.83118427 0.84462751 0.82612036 0.82488631 0.83247073\n",
      " 0.84724273 0.82514459 0.82488631 0.82514459 0.84523013 0.85641976\n",
      " 0.84970515 0.82531526 0.82841043 0.84296016 0.84022699 0.82856943\n",
      " 0.82488631 0.8280068  0.8444584  0.82586846 0.82488631 0.83012345\n",
      " 0.82531526 0.82488631 0.83552357 0.83997506 0.82488631 0.82488631\n",
      " 0.83886618 0.82488631 0.84227923 0.82488631 0.82488631 0.84395479\n",
      " 0.84326176 0.84123466 0.84214607 0.82488631 0.82488631 0.83247073\n",
      " 0.84221652 0.8554137  0.83384253 0.84494157 0.84123466 0.84045341\n",
      " 0.82940688 0.8278815  0.84303194 0.83861123 0.82488631 0.77844887\n",
      "        nan 0.83384253 0.82712333 0.84406458 0.83581309 0.82634772\n",
      " 0.84276655 0.8280068  0.83364647 0.77844887 0.82842473 0.83317591\n",
      " 0.77844887 0.83987605 0.8436945  0.83905714 0.83825509 0.84491204\n",
      " 0.83987605 0.84460501 0.82614502 0.82488631        nan 0.83133873\n",
      " 0.82488631 0.82586846 0.83247073 0.82939665 0.82488631 0.82488631\n",
      " 0.84276655 0.77844887 0.77844887 0.83581309 0.83304336 0.83504088\n",
      " 0.77844887 0.83026191 0.8280068  0.84649426 0.84143592 0.85537012\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.83253506 0.83480762\n",
      " 0.85461044 0.82514459 0.8450845  0.82488631 0.82488631 0.82488631\n",
      " 0.84540671 0.83630323 0.82488631 0.84276655 0.84324397 0.82488631\n",
      " 0.83242887 0.83979415 0.82627955 0.82488631 0.82488631 0.82488631\n",
      " 0.82514459 0.85941001 0.84422075 0.84276783 0.82488631 0.83581309\n",
      " 0.83581309 0.82514459 0.82488631 0.82488631 0.83986729 0.8535129\n",
      " 0.85421748 0.84394983 0.82841043 0.83581309        nan 0.84276783\n",
      " 0.8478552  0.77844887 0.85033441 0.77844887 0.82487201 0.82488631\n",
      " 0.83773101 0.8314787  0.84250571 0.82488631 0.82488631 0.82488631\n",
      " 0.83793687 0.83517315 0.84062648 0.82488631 0.82488631 0.82809486\n",
      " 0.8306114  0.82627955 0.83304336 0.84366284 0.82841236 0.84428996\n",
      " 0.82514459 0.84951859 0.84114733 0.82841043 0.83371772 0.82488631\n",
      " 0.8280068  0.82514459 0.83893887 0.8278815  0.83480762 0.77844887\n",
      " 0.84866035 0.83026191 0.82488631 0.82842473 0.8280068  0.84221652\n",
      " 0.84850477 0.82959425 0.82488631 0.83215973 0.82627955 0.82514459\n",
      " 0.82488631 0.82488631 0.83118427 0.83854423 0.83485431 0.8317569\n",
      " 0.84472615 0.84593807 0.82488631 0.82488631 0.82488631 0.84499722\n",
      " 0.82841043 0.82488631 0.84189061 0.83954218 0.77844887 0.82712333\n",
      " 0.84635714 0.84045341 0.8278815  0.82488631 0.84491204 0.83556602\n",
      " 0.84318959 0.82488631 0.83548537 0.83756799 0.84688479 0.83222366\n",
      " 0.82514459 0.82488631 0.82488631 0.82488631 0.83556602 0.82841043\n",
      " 0.82488631 0.84490555 0.8444584  0.82488631 0.84543013 0.84673915\n",
      " 0.82488631 0.84571515 0.82488631 0.83218559 0.82488631 0.82488631\n",
      " 0.83861123 0.83864066 0.82514459 0.8509793  0.82488631 0.83954218\n",
      " 0.82488631 0.82514459 0.82627955 0.82487201 0.85242791 0.77844887\n",
      " 0.84703301 0.84424201 0.8280068  0.82953066 0.82488631 0.82514459\n",
      " 0.82488631 0.82488631 0.8366722  0.82488631 0.84384648 0.77844887\n",
      " 0.82488631 0.83498151 0.83783511 0.82488631 0.82586846 0.83581309\n",
      " 0.83118427 0.82630227 0.8278815  0.82488631 0.82488631 0.8280068\n",
      " 0.83825509 0.83581309 0.83592061 0.82627955 0.84123466 0.841505\n",
      " 0.83304336 0.83581309 0.82488631 0.83581309 0.84859317 0.82488631\n",
      " 0.83267695 0.85553787        nan 0.84501705 0.82488631 0.82514459\n",
      " 0.77844887 0.85189016 0.84190699 0.83825509 0.82627955 0.84287178\n",
      " 0.8389098  0.84646872 0.82856943 0.84269216 0.82514459 0.82488631\n",
      " 0.82488631 0.82488631 0.84123466 0.84726516 0.82488631 0.82488631\n",
      " 0.82488631 0.84398494 0.84679758 0.83727502 0.83231997 0.82488631\n",
      "        nan 0.83162182 0.8317569  0.83752591 0.85193575 0.8278815\n",
      " 0.82488631 0.83954218 0.82488631 0.83158365 0.82488631 0.83987605\n",
      " 0.82488631 0.84920615 0.82488631 0.8278815  0.83987605 0.83924658\n",
      " 0.84460501 0.82488631 0.82488631        nan 0.83500369 0.84111541\n",
      " 0.83148343 0.82514459        nan 0.82712333 0.8429271  0.84224584\n",
      " 0.82488631 0.82634772 0.84133442 0.84276655 0.82488631 0.77844887\n",
      " 0.82488631 0.84921122 0.82488631 0.8278815  0.83893887 0.82488631\n",
      " 0.82904928 0.84052152 0.82488631 0.82627955 0.83700681 0.82488631\n",
      " 0.84417704 0.82488631 0.8280068  0.84014159 0.82488631 0.83332056\n",
      " 0.82627955 0.83997506 0.8278815  0.82627955        nan 0.83617917\n",
      " 0.83231997 0.84711829 0.82488631 0.82488631 0.84823064 0.82627955\n",
      " 0.83856831 0.83148343 0.82488631 0.82488631 0.84452754 0.8360193\n",
      " 0.83270994 0.77844887 0.82345437 0.77844887 0.84188183 0.83762658\n",
      " 0.82926049 0.83364647 0.84475804 0.82488631 0.82627955 0.830101\n",
      " 0.83478135 0.82488631 0.77844887 0.84880461 0.82488631 0.8447298\n",
      " 0.84133442 0.77844887 0.83581309 0.77844887 0.77844887 0.82926049\n",
      " 0.82488631 0.85308279 0.82855513 0.82841043        nan 0.82842473\n",
      " 0.83384253 0.82514459 0.83861123 0.8268126  0.83556602 0.84823064\n",
      " 0.82488631 0.83581309 0.82488631 0.77844887 0.82514459 0.84221652\n",
      " 0.84381077 0.8450615  0.82488631 0.84081904 0.84649426 0.82627955\n",
      " 0.84525968 0.82904928 0.82514459 0.82488631 0.84593807 0.82488631\n",
      " 0.84462751 0.82488631 0.82627955 0.83517315 0.83026191 0.82875603\n",
      " 0.84984406 0.83433522 0.82644261 0.82856943 0.83886504 0.82514459\n",
      " 0.82514459 0.82627955 0.82488631 0.82488631 0.82488631 0.82488631\n",
      " 0.83924658 0.82627955 0.84795638 0.84724289 0.82881207        nan\n",
      " 0.82488631 0.82488631        nan 0.84269842 0.83076301        nan\n",
      " 0.83865213 0.82904928 0.83077653 0.82940688 0.84673915 0.82896341\n",
      " 0.83077653 0.8280068  0.830101   0.84523013 0.83997506 0.83987605\n",
      " 0.83834604 0.84494157 0.8478552  0.82488631 0.83942426 0.83304336\n",
      " 0.84189061 0.85020423 0.84574416 0.83026191 0.82488631 0.84390462\n",
      " 0.82488631 0.84794828]\n",
      "  warnings.warn(\n",
      "c:\\Users\\mukes\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.85631015 0.77774902 0.84140421 0.85280786 0.85178595 0.87113298\n",
      " 0.82583601 0.8367705  0.84981229 0.84415784 0.82583601 0.83861476\n",
      " 0.87696169 0.82851932 0.82583601 0.82851932 0.85216545 0.88289214\n",
      " 0.86003146 0.84695533 0.83235552 0.86891607 0.84800694 0.82973987\n",
      " 0.82583601 0.83272552 0.85778839 0.83080794 0.82583601 0.83384655\n",
      " 0.84695533 0.82583601 0.84255584 0.84827272 0.82583601 0.82583601\n",
      " 0.85969102 0.82583601 0.87819379 0.82583601 0.82583601 0.85661873\n",
      " 0.86357122 0.85036837 0.86461882 0.82583601 0.82583601 0.83861476\n",
      " 0.84752902 0.88159006 0.84022015 0.85316171 0.85019419 0.84967888\n",
      " 0.83451486 0.83301112 0.8584718  0.84625332 0.82583601 0.77774902\n",
      "        nan 0.84022015 0.84850853 0.87420507 0.84238417 0.84140421\n",
      " 0.84810619 0.83272552 0.83971985 0.77774902 0.83004236 0.83923703\n",
      " 0.77774902 0.84373893 0.85760164 0.86046327 0.84786933 0.85280786\n",
      " 0.84373893 0.85761337 0.8462253  0.82583601        nan 0.83634263\n",
      " 0.82583601 0.83024509 0.83861476 0.86362211 0.82583601 0.82583601\n",
      " 0.84810619 0.77774902 0.77774902 0.84238417 0.84049152 0.8685631\n",
      " 0.77774902 0.83261379 0.83272552 0.85498969 0.85178595 0.86875616\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.85314106 0.84214072\n",
      " 0.87405456 0.82848866 0.85286541 0.82583601 0.82583601 0.82583601\n",
      " 0.85778124 0.87705888 0.82583601 0.84810619 0.86310374 0.82583601\n",
      " 0.85252173 0.86310741 0.83082791 0.82583601 0.82583601 0.82583601\n",
      " 0.82851932 0.88350967 0.85141261 0.85471897 0.82583601 0.84238417\n",
      " 0.84238417 0.82851932 0.82583601 0.82583601 0.85077011 0.87590036\n",
      " 0.87214664 0.87256278 0.83235552 0.84238417        nan 0.85471897\n",
      " 0.87035354 0.77774902 0.86094554 0.77774902 0.82814917 0.82583601\n",
      " 0.84818137 0.85510827 0.84772977 0.82583601 0.82583601 0.82583601\n",
      " 0.86166173 0.85745189 0.84469235 0.82583601 0.82583601 0.86108877\n",
      " 0.85681927 0.83082791 0.84049152 0.86956326 0.852295   0.84936169\n",
      " 0.82851932 0.86648145 0.86182457 0.83235552 0.8406268  0.82583601\n",
      " 0.83272552 0.82851932 0.84532065 0.83301112 0.84214072 0.77774902\n",
      " 0.86278588 0.83261379 0.82583601 0.83004236 0.83272552 0.84744981\n",
      " 0.85766795 0.83445936 0.82583601 0.83764329 0.83082791 0.82851932\n",
      " 0.82583601 0.82583601 0.8367705  0.87071501 0.87331034 0.83864725\n",
      " 0.87490211 0.85287561 0.82583601 0.82583601 0.82583601 0.85228415\n",
      " 0.83235552 0.82583601 0.84732778 0.84715573 0.77774902 0.84850853\n",
      " 0.86416085 0.84967888 0.83301112 0.82583601 0.85280786 0.8428255\n",
      " 0.86283439 0.82583601 0.84597667 0.852871   0.86102433 0.83905609\n",
      " 0.82851932 0.82583601 0.82583601 0.82583601 0.8428255  0.83235552\n",
      " 0.82583601 0.86700547 0.85778839 0.82583601 0.86407617 0.85676346\n",
      " 0.82583601 0.87252218 0.82583601 0.8379911  0.82583601 0.82583601\n",
      " 0.84625332 0.8470861  0.82851932 0.87551574 0.82583601 0.84732028\n",
      " 0.82583601 0.82851932 0.83082791 0.82814917 0.86537303 0.77774902\n",
      " 0.85566797 0.85151501 0.83272552 0.83498224 0.82583601 0.82851932\n",
      " 0.82583601 0.82583601 0.84182    0.82583601 0.86817496 0.77774902\n",
      " 0.82583601 0.85834674 0.85602064 0.82583601 0.83030851 0.84238417\n",
      " 0.8367705  0.82858492 0.83301112 0.82583601 0.82583601 0.83272552\n",
      " 0.84786933 0.84238417 0.85404545 0.83082791 0.85036837 0.86768258\n",
      " 0.84049152 0.84238417 0.82583601 0.84238417 0.86359517 0.82583601\n",
      " 0.84031158 0.87503805        nan 0.85475218 0.82583601 0.82851932\n",
      " 0.77774902 0.88623353 0.86631783 0.84786933 0.83082791 0.85547508\n",
      " 0.84840174 0.85979911 0.82973987 0.84997759 0.82851932 0.82583601\n",
      " 0.82583601 0.82583601 0.85019419 0.87368513 0.82583601 0.82583601\n",
      " 0.82583601 0.85713571 0.85496571 0.84616532 0.8394209  0.82583601\n",
      "        nan 0.84242238 0.83864725 0.86190533 0.8713187  0.83301112\n",
      " 0.82583601 0.84732028 0.82583601 0.85918921 0.82583601 0.84373893\n",
      " 0.82583601 0.857924   0.82583601 0.83301112 0.84373893 0.86253741\n",
      " 0.85761337 0.82583601 0.82583601        nan 0.84257766 0.86207899\n",
      " 0.83617732 0.82851932        nan 0.8486979  0.85378376 0.85416438\n",
      " 0.82583601 0.84140421 0.84540251 0.84810619 0.82583601 0.77774902\n",
      " 0.82583601 0.86213244 0.82583601 0.83301112 0.84532065 0.82583601\n",
      " 0.83272057 0.85708659 0.82583601 0.83082791 0.85571576 0.82583601\n",
      " 0.86503945 0.82583601 0.83272552 0.8622256  0.82583601 0.83977078\n",
      " 0.83082791 0.84827272 0.83301112 0.83082791        nan 0.84412918\n",
      " 0.8394209  0.8801963  0.82583601 0.82583601 0.86349013 0.83082791\n",
      " 0.8437397  0.83617732 0.82583601 0.82583601 0.85104248 0.84408099\n",
      " 0.86295287 0.77774902 0.8653161  0.77774902 0.86871881 0.84496355\n",
      " 0.83397202 0.83978321 0.85342046 0.82583601 0.83082791 0.83542494\n",
      " 0.85686556 0.82583601 0.77774902 0.86268894 0.82583601 0.86258614\n",
      " 0.84540251 0.77774902 0.84238417 0.77774902 0.77774902 0.83452251\n",
      " 0.82583601 0.87766869 0.83202072 0.83235552        nan 0.83004236\n",
      " 0.84015679 0.82851932 0.84625332 0.83299591 0.8428255  0.86356014\n",
      " 0.82583601 0.84238417 0.82583601 0.77774902 0.82851932 0.84752902\n",
      " 0.85694604 0.8598196  0.82583601 0.85985402 0.85480087 0.83082791\n",
      " 0.85360388 0.83272057 0.82851932 0.82583601 0.85287561 0.82583601\n",
      " 0.84981229 0.82583601 0.83082791 0.85745189 0.83278796 0.84193843\n",
      " 0.87084695 0.83790805 0.85769243 0.82957533 0.86025848 0.82851932\n",
      " 0.82851932 0.83082791 0.82583601 0.82583601 0.82583601 0.82583601\n",
      " 0.86253741 0.83082791 0.8637977  0.86773561 0.85762767        nan\n",
      " 0.82583601 0.82583601        nan 0.86224713 0.84076391        nan\n",
      " 0.86268011 0.83272057 0.83461817 0.83445144 0.85701515 0.85839546\n",
      " 0.83461817 0.83272552 0.83542494 0.85216545 0.84827272 0.84373893\n",
      " 0.84634707 0.85318962 0.87035354 0.82583601 0.85623245 0.84049152\n",
      " 0.84767897 0.86700072 0.8526681  0.83261379 0.82583601 0.87572246\n",
      " 0.82583601 0.85864035]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,42),  \n",
    "    'min_samples_leaf': np.arange(1,42),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 42), \n",
    "    'max_depth': np.arange(1,42), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct an exhaustive search across a smaller range of parameters around the parameters found in the initial random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "The best precision score is 0.860525854957516\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 39, 'max_leaf_nodes': 36, 'min_impurity_decrease': 0.0009, 'min_samples_leaf': 5, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(3,6),  \n",
    "    'min_samples_leaf': np.arange(3,6),\n",
    "    'min_impurity_decrease': np.arange(0.0009, 0.0012,0.0001),\n",
    "    'max_leaf_nodes': np.arange(36,40), \n",
    "    'max_depth': np.arange(39,42), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 4 models,\n",
    "\n",
    "it appears that the SVM model with a polynomial kernel and either random or grid search performed better than the decision tree models in terms of precision score. The SVM model achieved a precision score of 0.93, while the decision tree models achieved lower precision scores of 0.85 and 0.86.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "51304e485c3ea139da54eebdcffd6e2e412a5cc3707a99f593ae18b19213128f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
